{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PBpKmf19fBK"
   },
   "source": [
    "### cardiffnlp/twitter-roberta-base-sentiment\n",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment?text=I+like+you.+I+love+you\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No. 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EkugXyMh3T9W",
    "outputId": "0cb25cfa-49ef-4bb2-b0e8-1c2ad227487a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /Users/qiguo/.cache/huggingface/hub/models--cardiffnlp--twitter-roberta-base-sentiment/snapshots/daefdd1f6ae931839bce4d0f3db0a1a4265cd50f/vocab.json\n",
      "loading file merges.txt from cache at /Users/qiguo/.cache/huggingface/hub/models--cardiffnlp--twitter-roberta-base-sentiment/snapshots/daefdd1f6ae931839bce4d0f3db0a1a4265cd50f/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/qiguo/.cache/huggingface/hub/models--cardiffnlp--twitter-roberta-base-sentiment/snapshots/daefdd1f6ae931839bce4d0f3db0a1a4265cd50f/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /Users/qiguo/.cache/huggingface/hub/models--cardiffnlp--twitter-roberta-base-sentiment/snapshots/daefdd1f6ae931839bce4d0f3db0a1a4265cd50f/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-sentiment\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1838349e7ab472ea65245a55f102a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee78291789f74580bb5887ca3ebc8a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da60d662d44f4c78ad69b9dea8e35623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/qiguo/.cache/huggingface/hub/models--cardiffnlp--twitter-roberta-base-sentiment/snapshots/daefdd1f6ae931839bce4d0f3db0a1a4265cd50f/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"tweeteval_new/roberta-base-rt-sentiment/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/qiguo/.cache/huggingface/hub/models--cardiffnlp--twitter-roberta-base-sentiment/snapshots/daefdd1f6ae931839bce4d0f3db0a1a4265cd50f/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 6184\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3870\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3096' max='3870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3096/3870 1:32:52 < 23:14, 0.56 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.195500</td>\n",
       "      <td>0.110933</td>\n",
       "      <td>0.970246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.071500</td>\n",
       "      <td>0.122635</td>\n",
       "      <td>0.972833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>0.194745</td>\n",
       "      <td>0.967658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189034</td>\n",
       "      <td>0.974127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.215529</td>\n",
       "      <td>0.972833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.229271</td>\n",
       "      <td>0.975420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.242361</td>\n",
       "      <td>0.970246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.264584</td>\n",
       "      <td>0.971539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 773\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_1/checkpoint-387\n",
      "Configuration saved in ./results_1/checkpoint-387/config.json\n",
      "Model weights saved in ./results_1/checkpoint-387/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_1/checkpoint-387/tokenizer_config.json\n",
      "Special tokens file saved in ./results_1/checkpoint-387/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 773\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_1/checkpoint-774\n",
      "Configuration saved in ./results_1/checkpoint-774/config.json\n",
      "Model weights saved in ./results_1/checkpoint-774/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_1/checkpoint-774/tokenizer_config.json\n",
      "Special tokens file saved in ./results_1/checkpoint-774/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 773\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_1/checkpoint-1161\n",
      "Configuration saved in ./results_1/checkpoint-1161/config.json\n",
      "Model weights saved in ./results_1/checkpoint-1161/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_1/checkpoint-1161/tokenizer_config.json\n",
      "Special tokens file saved in ./results_1/checkpoint-1161/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 773\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_1/checkpoint-1548\n",
      "Configuration saved in ./results_1/checkpoint-1548/config.json\n",
      "Model weights saved in ./results_1/checkpoint-1548/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_1/checkpoint-1548/tokenizer_config.json\n",
      "Special tokens file saved in ./results_1/checkpoint-1548/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 773\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_1/checkpoint-1935\n",
      "Configuration saved in ./results_1/checkpoint-1935/config.json\n",
      "Model weights saved in ./results_1/checkpoint-1935/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_1/checkpoint-1935/tokenizer_config.json\n",
      "Special tokens file saved in ./results_1/checkpoint-1935/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 773\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_1/checkpoint-2322\n",
      "Configuration saved in ./results_1/checkpoint-2322/config.json\n",
      "Model weights saved in ./results_1/checkpoint-2322/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_1/checkpoint-2322/tokenizer_config.json\n",
      "Special tokens file saved in ./results_1/checkpoint-2322/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 773\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_1/checkpoint-2709\n",
      "Configuration saved in ./results_1/checkpoint-2709/config.json\n",
      "Model weights saved in ./results_1/checkpoint-2709/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_1/checkpoint-2709/tokenizer_config.json\n",
      "Special tokens file saved in ./results_1/checkpoint-2709/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 773\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_1/checkpoint-3096\n",
      "Configuration saved in ./results_1/checkpoint-3096/config.json\n",
      "Model weights saved in ./results_1/checkpoint-3096/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_1/checkpoint-3096/tokenizer_config.json\n",
      "Special tokens file saved in ./results_1/checkpoint-3096/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_1/checkpoint-2322 (score: 0.9754204154014587).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 773\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 774\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22927097976207733, 'eval_accuracy': 0.9754204154014587, 'eval_runtime': 23.3994, 'eval_samples_per_second': 33.035, 'eval_steps_per_second': 2.094, 'epoch': 8.0}\n",
      "Test metrics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./trained_model_1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2054007351398468, 'eval_accuracy': 0.9767441749572754, 'eval_runtime': 23.1532, 'eval_samples_per_second': 33.43, 'eval_steps_per_second': 2.116, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./trained_model_1/pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model_1/tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model_1/special_tokens_map.json\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 774\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification distribution:\n",
      "label\n",
      "1    13\n",
      "0     5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import os\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('Preprocessed_Dataset/train.csv')\n",
    "val_df = pd.read_csv('Preprocessed_Dataset/dev.csv')\n",
    "test_df = pd.read_csv('Preprocessed_Dataset/test.csv')\n",
    "\n",
    "# Convert DataFrame to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Merge datasets\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Tokenizer\n",
    "model_checkpoint = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Tokenize\n",
    "def preprocess_function(examples):\n",
    "    cleaned_texts = [str(text) if text else \"\" for text in examples[\"text\"]]\n",
    "    return tokenizer(cleaned_texts, truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Load the model\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2, ignore_mismatched_sizes=True)  # 2 classes\n",
    "\n",
    "# Data Collector\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# Evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = torch.argmax(torch.tensor(predictions), dim=-1)\n",
    "    accuracy = (preds == torch.tensor(labels)).float().mean().item()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_1',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# learning rate scheduler\n",
    "num_training_steps = len(tokenized_datasets[\"train\"]) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=num_training_steps)\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2) \n",
    "\n",
    "# Trainer class\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Validation metrics:\")\n",
    "val_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n",
    "print(val_metrics)\n",
    "\n",
    "print(\"Test metrics:\")\n",
    "test_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "print(test_metrics)\n",
    "\n",
    "# Save\n",
    "model.save_pretrained('./trained_model_1')\n",
    "tokenizer.save_pretrained('./trained_model_1')\n",
    "\n",
    "# Get predictions for the test set using the trained model\n",
    "def get_predictions_and_logits(trainer, dataset):\n",
    "    predictions, labels, metrics = trainer.predict(dataset)\n",
    "    \n",
    "    # Convert logits to predicted labels (0 or 1)\n",
    "    predicted_labels = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    return predictions, predicted_labels, labels\n",
    "\n",
    "# Get predictions, logits, and true labels for the test dataset\n",
    "predictions, predicted_labels, true_labels = get_predictions_and_logits(trainer, tokenized_datasets[\"test\"])\n",
    "\n",
    "# Convert logits to pandas DataFrame for easy manipulation\n",
    "logits_df = pd.DataFrame(predictions, columns=[\"logit_\" + str(i) for i in range(predictions.shape[1])])\n",
    "\n",
    "# Create a DataFrame with text, true labels, and predicted labels\n",
    "results_df = pd.DataFrame({\n",
    "    'text': tokenized_datasets[\"test\"][\"text\"],\n",
    "    'label': true_labels,\n",
    "    'label_pred': predicted_labels\n",
    "})\n",
    "\n",
    "# Concatenate logits DataFrame with the results_df\n",
    "final_df = pd.concat([results_df, logits_df], axis=1)\n",
    "final_df.to_csv(\"test_predictions_with_logits_1.csv\", index=False)\n",
    "\n",
    "# Filter out misclassified samples\n",
    "misclassified_df = results_df[results_df['label'] != results_df['label_pred']]\n",
    "misclassified_df.to_csv('misclassified_samples_1.csv', index=False)\n",
    "\n",
    "# Count per label\n",
    "misclassified_counts = misclassified_df['label'].value_counts()\n",
    "\n",
    "# Display distribution\n",
    "print(f\"Misclassification distribution:\\n{misclassified_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835668c0e922417b9031a54d65204e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55142729d3bd4e91911f9690306dcea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789abb0ae6914dffb064860906a34ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./trained_model_1/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-sentiment\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./trained_model_1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./trained_model_1 and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 6006\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3760\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3760' max='3760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3760/3760 2:30:05, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.762100</td>\n",
       "      <td>0.746026</td>\n",
       "      <td>0.649689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.637900</td>\n",
       "      <td>0.573149</td>\n",
       "      <td>0.751779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.556200</td>\n",
       "      <td>0.481160</td>\n",
       "      <td>0.818283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.370100</td>\n",
       "      <td>0.395371</td>\n",
       "      <td>0.870774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.345600</td>\n",
       "      <td>0.510208</td>\n",
       "      <td>0.864101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.200100</td>\n",
       "      <td>0.554717</td>\n",
       "      <td>0.885899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>0.677809</td>\n",
       "      <td>0.887233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.070500</td>\n",
       "      <td>0.700173</td>\n",
       "      <td>0.891459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.114200</td>\n",
       "      <td>0.702712</td>\n",
       "      <td>0.900356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.708053</td>\n",
       "      <td>0.898576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_3/checkpoint-376\n",
      "Configuration saved in ./results_3/checkpoint-376/config.json\n",
      "Model weights saved in ./results_3/checkpoint-376/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_3/checkpoint-376/tokenizer_config.json\n",
      "Special tokens file saved in ./results_3/checkpoint-376/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_3/checkpoint-752\n",
      "Configuration saved in ./results_3/checkpoint-752/config.json\n",
      "Model weights saved in ./results_3/checkpoint-752/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_3/checkpoint-752/tokenizer_config.json\n",
      "Special tokens file saved in ./results_3/checkpoint-752/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_3/checkpoint-1128\n",
      "Configuration saved in ./results_3/checkpoint-1128/config.json\n",
      "Model weights saved in ./results_3/checkpoint-1128/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_3/checkpoint-1128/tokenizer_config.json\n",
      "Special tokens file saved in ./results_3/checkpoint-1128/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_3/checkpoint-1504\n",
      "Configuration saved in ./results_3/checkpoint-1504/config.json\n",
      "Model weights saved in ./results_3/checkpoint-1504/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_3/checkpoint-1504/tokenizer_config.json\n",
      "Special tokens file saved in ./results_3/checkpoint-1504/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_3/checkpoint-1880\n",
      "Configuration saved in ./results_3/checkpoint-1880/config.json\n",
      "Model weights saved in ./results_3/checkpoint-1880/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_3/checkpoint-1880/tokenizer_config.json\n",
      "Special tokens file saved in ./results_3/checkpoint-1880/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_3/checkpoint-2256\n",
      "Configuration saved in ./results_3/checkpoint-2256/config.json\n",
      "Model weights saved in ./results_3/checkpoint-2256/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_3/checkpoint-2256/tokenizer_config.json\n",
      "Special tokens file saved in ./results_3/checkpoint-2256/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_3/checkpoint-2632\n",
      "Configuration saved in ./results_3/checkpoint-2632/config.json\n",
      "Model weights saved in ./results_3/checkpoint-2632/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_3/checkpoint-2632/tokenizer_config.json\n",
      "Special tokens file saved in ./results_3/checkpoint-2632/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_3/checkpoint-3008\n",
      "Configuration saved in ./results_3/checkpoint-3008/config.json\n",
      "Model weights saved in ./results_3/checkpoint-3008/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_3/checkpoint-3008/tokenizer_config.json\n",
      "Special tokens file saved in ./results_3/checkpoint-3008/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_3/checkpoint-3384\n",
      "Configuration saved in ./results_3/checkpoint-3384/config.json\n",
      "Model weights saved in ./results_3/checkpoint-3384/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_3/checkpoint-3384/tokenizer_config.json\n",
      "Special tokens file saved in ./results_3/checkpoint-3384/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_3/checkpoint-3760\n",
      "Configuration saved in ./results_3/checkpoint-3760/config.json\n",
      "Model weights saved in ./results_3/checkpoint-3760/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_3/checkpoint-3760/tokenizer_config.json\n",
      "Special tokens file saved in ./results_3/checkpoint-3760/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_3/checkpoint-3384 (score: 0.900355875492096).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3245\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7027115225791931, 'eval_accuracy': 0.900355875492096, 'eval_runtime': 161.3989, 'eval_samples_per_second': 27.856, 'eval_steps_per_second': 1.741, 'epoch': 10.0}\n",
      "Test metrics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./trained_model_3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.62990665435791, 'eval_accuracy': 0.6258859634399414, 'eval_runtime': 115.8796, 'eval_samples_per_second': 28.003, 'eval_steps_per_second': 1.752, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./trained_model_3/pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model_3/tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model_3/special_tokens_map.json\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3245\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification distribution:\n",
      "label\n",
      "1    660\n",
      "2    411\n",
      "0    143\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('LT-EDI-ACL2022_Dataset/train.csv')\n",
    "dev_df = pd.read_csv('LT-EDI-ACL2022_Dataset/dev.tsv', sep='\\t')\n",
    "test_df = pd.read_csv('LT-EDI-ACL2022_Dataset/test.csv')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_df.drop(\"pid\", axis=1, inplace=True)\n",
    "dev_df.drop(\"PID\", axis=1, inplace=True)\n",
    "test_df.drop(\"pid\", axis=1, inplace=True)\n",
    "\n",
    "# Rename columns for dev_df\n",
    "dev_df = dev_df.rename(columns={'Text data': 'text', 'Label': 'labels'})\n",
    "\n",
    "# Map label names to integers\n",
    "label_mapping = {'severe': 0, 'moderate': 1, 'not depression': 2}\n",
    "dev_df['labels'] = dev_df['labels'].map(label_mapping)\n",
    "\n",
    "# Convert DataFrame to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(dev_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Merge datasets into a DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Tokenizer: Load from last training\n",
    "model_checkpoint = './trained_model_1'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    cleaned_texts = [str(text) if text else \"\" for text in examples[\"text\"]]\n",
    "    return tokenizer(cleaned_texts, truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Load the base model (Roberta base) for sequence classification with 3 classes\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Data Collator: Use padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# Define evaluation metrics (accuracy)\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = torch.argmax(torch.tensor(predictions), dim=-1)\n",
    "    accuracy = (preds == torch.tensor(labels)).float().mean().item()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_3\",  # Updated output directory\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_3',  # Updated logging directory\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Scheduler for learning rate\n",
    "num_training_steps = len(tokenized_datasets[\"train\"]) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "# Trainer class\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Validation metrics:\")\n",
    "val_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n",
    "print(val_metrics)\n",
    "\n",
    "print(\"Test metrics:\")\n",
    "test_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "print(test_metrics)\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "model.save_pretrained('./trained_model_3')\n",
    "tokenizer.save_pretrained('./trained_model_3')\n",
    "\n",
    "# Get predictions for the test set using the trained model\n",
    "def get_predictions_and_logits(trainer, dataset):\n",
    "    predictions, labels, metrics = trainer.predict(dataset)\n",
    "    \n",
    "    # Convert logits to predicted labels \n",
    "    predicted_labels = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    return predictions, predicted_labels, labels\n",
    "\n",
    "# Get predictions, logits, and true labels for the test dataset\n",
    "predictions, predicted_labels, true_labels = get_predictions_and_logits(trainer, tokenized_datasets[\"test\"])\n",
    "\n",
    "# Convert logits to pandas DataFrame for easy manipulation\n",
    "logits_df = pd.DataFrame(predictions, columns=[\"logit_\" + str(i) for i in range(predictions.shape[1])])\n",
    "\n",
    "# Create a DataFrame with text, true labels, and predicted labels\n",
    "results_df = pd.DataFrame({\n",
    "    'text': tokenized_datasets[\"test\"][\"text\"],\n",
    "    'label': true_labels,\n",
    "    'label_pred': predicted_labels\n",
    "})\n",
    "\n",
    "# Concatenate logits DataFrame with the results_df\n",
    "final_df = pd.concat([results_df, logits_df], axis=1)\n",
    "final_df.to_csv(\"test_predictions_with_logits_3.csv\", index=False)\n",
    "\n",
    "# Filter out misclassified samples\n",
    "misclassified_df = results_df[results_df['label'] != results_df['label_pred']]\n",
    "misclassified_df.to_csv('misclassified_samples_3.csv', index=False)\n",
    "\n",
    "# Count per label\n",
    "misclassified_counts = misclassified_df['label'].value_counts()\n",
    "\n",
    "# Display distribution of misclassified labels\n",
    "print(f\"Misclassification distribution:\\n{misclassified_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02e5c984c09f435a8c9ad7fee9c0a3b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5e5b1574b89f4266bf5069a75bfd1259",
       "IPY_MODEL_6b969a0ec21d44afbb294752e0829879",
       "IPY_MODEL_5e79822663234199bbe63f1bb3b07739"
      ],
      "layout": "IPY_MODEL_27888e6192c74a13b51525f5b069246c"
     }
    },
    "18ef0f43fc8c407db9e93fc2eb456f09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1d5cf447c04047a5b763e2258918a5a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1fac47e82d8445fe9c061c5c817b7c07": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27888e6192c74a13b51525f5b069246c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f0301ad571c43f4a3f6c74ee11478ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4184657ba94e4390a12546a75189b23c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f0d6b2d68e24d4f92eecdf40266dbb5",
      "placeholder": "",
      "style": "IPY_MODEL_1d5cf447c04047a5b763e2258918a5a3",
      "value": "774/774[00:00&lt;00:00,1778.11examples/s]"
     }
    },
    "43d38d94d64843c2822fcb105c44b9ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49adb58f54964934ab25de27be100fb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c8fdd7ab5e5412989b1eebed2cb0f32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "570858b160114bd7989322f8171be5f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e5b1574b89f4266bf5069a75bfd1259": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f0301ad571c43f4a3f6c74ee11478ae",
      "placeholder": "",
      "style": "IPY_MODEL_920685d2c4524821aa53546cb15ca4cc",
      "value": "Map:100%"
     }
    },
    "5e79822663234199bbe63f1bb3b07739": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fac47e82d8445fe9c061c5c817b7c07",
      "placeholder": "",
      "style": "IPY_MODEL_8aed4c454d1f4264aaabaa1d9df58012",
      "value": "773/773[00:00&lt;00:00,1801.24examples/s]"
     }
    },
    "689163b20cda4097ace66a99862d6c61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_947d37a10f674733897f7c062feff1e6",
      "max": 6184,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_799015bd31c8441f9de61f77bd381563",
      "value": 6184
     }
    },
    "6b969a0ec21d44afbb294752e0829879": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49adb58f54964934ab25de27be100fb2",
      "max": 773,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c248980abfef4874ab0003a5c5fe767d",
      "value": 773
     }
    },
    "6da44f13a284451ca8c4bf68c3d8b421": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f92da9b60b0b495f8b937c9bda1c7f6a",
       "IPY_MODEL_689163b20cda4097ace66a99862d6c61",
       "IPY_MODEL_fabf53cd91ca4a1e89567bf7c6d47c49"
      ],
      "layout": "IPY_MODEL_8d3f09e85c57469fbbe12a566097431b"
     }
    },
    "799015bd31c8441f9de61f77bd381563": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8266a18427f249d4b25c6ac3ff87f33b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "85fbfe4074c1472ea5d67c9cffabb38c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87ee335a83a24c3d9df0668f5590f94b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c4679acd51ed487db62902f93b68597d",
       "IPY_MODEL_f38daed0ecee4e5f93f95435feca0992",
       "IPY_MODEL_4184657ba94e4390a12546a75189b23c"
      ],
      "layout": "IPY_MODEL_abd7f1c0737247e48a2862bbbee86b74"
     }
    },
    "8aed4c454d1f4264aaabaa1d9df58012": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d3f09e85c57469fbbe12a566097431b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f0d6b2d68e24d4f92eecdf40266dbb5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "920685d2c4524821aa53546cb15ca4cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "947d37a10f674733897f7c062feff1e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abd7f1c0737247e48a2862bbbee86b74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c248980abfef4874ab0003a5c5fe767d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c4679acd51ed487db62902f93b68597d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d764d603b0704c9890291f2e2db0e76a",
      "placeholder": "",
      "style": "IPY_MODEL_570858b160114bd7989322f8171be5f5",
      "value": "Map:100%"
     }
    },
    "d764d603b0704c9890291f2e2db0e76a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da47dee2ce5d47c99e5b55d2e61be352": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f38daed0ecee4e5f93f95435feca0992": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43d38d94d64843c2822fcb105c44b9ff",
      "max": 774,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8266a18427f249d4b25c6ac3ff87f33b",
      "value": 774
     }
    },
    "f92da9b60b0b495f8b937c9bda1c7f6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85fbfe4074c1472ea5d67c9cffabb38c",
      "placeholder": "",
      "style": "IPY_MODEL_da47dee2ce5d47c99e5b55d2e61be352",
      "value": "Map:100%"
     }
    },
    "fabf53cd91ca4a1e89567bf7c6d47c49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c8fdd7ab5e5412989b1eebed2cb0f32",
      "placeholder": "",
      "style": "IPY_MODEL_18ef0f43fc8c407db9e93fc2eb456f09",
      "value": "6184/6184[00:02&lt;00:00,2540.53examples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
