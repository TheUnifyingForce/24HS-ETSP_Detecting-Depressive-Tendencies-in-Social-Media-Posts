{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FacebookAI/roberta-base\n",
    "https://huggingface.co/FacebookAI/roberta-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No. 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /Users/qiguo/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json\n",
      "loading file merges.txt from cache at /Users/qiguo/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/qiguo/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/qiguo/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc91a310aa6446db25cb7a3385073f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95dcf9b740cd4f98a8312bb812653978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef976e5844d43e6ada0344fb88890de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/qiguo/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/qiguo/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 6006\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5640\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5640' max='5640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5640/5640 3:32:33, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.778300</td>\n",
       "      <td>0.714310</td>\n",
       "      <td>0.669706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.678300</td>\n",
       "      <td>0.612722</td>\n",
       "      <td>0.731317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.665100</td>\n",
       "      <td>0.564246</td>\n",
       "      <td>0.771575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.486200</td>\n",
       "      <td>0.461222</td>\n",
       "      <td>0.848754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.433100</td>\n",
       "      <td>0.503167</td>\n",
       "      <td>0.836966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.336600</td>\n",
       "      <td>0.596253</td>\n",
       "      <td>0.860543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.615508</td>\n",
       "      <td>0.877224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>0.749697</td>\n",
       "      <td>0.889680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.114300</td>\n",
       "      <td>0.788997</td>\n",
       "      <td>0.885676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.815830</td>\n",
       "      <td>0.890347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.877755</td>\n",
       "      <td>0.888345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.888621</td>\n",
       "      <td>0.893906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.930655</td>\n",
       "      <td>0.890347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.916851</td>\n",
       "      <td>0.894351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.928108</td>\n",
       "      <td>0.894795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_6/checkpoint-376\n",
      "Configuration saved in ./results_6/checkpoint-376/config.json\n",
      "Model weights saved in ./results_6/checkpoint-376/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_6/checkpoint-376/tokenizer_config.json\n",
      "Special tokens file saved in ./results_6/checkpoint-376/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_6/checkpoint-752\n",
      "Configuration saved in ./results_6/checkpoint-752/config.json\n",
      "Model weights saved in ./results_6/checkpoint-752/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_6/checkpoint-752/tokenizer_config.json\n",
      "Special tokens file saved in ./results_6/checkpoint-752/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_6/checkpoint-1128\n",
      "Configuration saved in ./results_6/checkpoint-1128/config.json\n",
      "Model weights saved in ./results_6/checkpoint-1128/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_6/checkpoint-1128/tokenizer_config.json\n",
      "Special tokens file saved in ./results_6/checkpoint-1128/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_6/checkpoint-1504\n",
      "Configuration saved in ./results_6/checkpoint-1504/config.json\n",
      "Model weights saved in ./results_6/checkpoint-1504/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_6/checkpoint-1504/tokenizer_config.json\n",
      "Special tokens file saved in ./results_6/checkpoint-1504/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_6/checkpoint-1880\n",
      "Configuration saved in ./results_6/checkpoint-1880/config.json\n",
      "Model weights saved in ./results_6/checkpoint-1880/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_6/checkpoint-1880/tokenizer_config.json\n",
      "Special tokens file saved in ./results_6/checkpoint-1880/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_6/checkpoint-2256\n",
      "Configuration saved in ./results_6/checkpoint-2256/config.json\n",
      "Model weights saved in ./results_6/checkpoint-2256/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_6/checkpoint-2256/tokenizer_config.json\n",
      "Special tokens file saved in ./results_6/checkpoint-2256/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_6/checkpoint-2632\n",
      "Configuration saved in ./results_6/checkpoint-2632/config.json\n",
      "Model weights saved in ./results_6/checkpoint-2632/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_6/checkpoint-2632/tokenizer_config.json\n",
      "Special tokens file saved in ./results_6/checkpoint-2632/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_6/checkpoint-3008\n",
      "Configuration saved in ./results_6/checkpoint-3008/config.json\n",
      "Model weights saved in ./results_6/checkpoint-3008/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_6/checkpoint-3008/tokenizer_config.json\n",
      "Special tokens file saved in ./results_6/checkpoint-3008/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_6/checkpoint-3384\n",
      "Configuration saved in ./results_6/checkpoint-3384/config.json\n",
      "Model weights saved in ./results_6/checkpoint-3384/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_6/checkpoint-3384/tokenizer_config.json\n",
      "Special tokens file saved in ./results_6/checkpoint-3384/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_6/checkpoint-3760\n",
      "Configuration saved in ./results_6/checkpoint-3760/config.json\n",
      "Model weights saved in ./results_6/checkpoint-3760/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_6/checkpoint-3760/tokenizer_config.json\n",
      "Special tokens file saved in ./results_6/checkpoint-3760/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_6/checkpoint-4136\n",
      "Configuration saved in ./results_6/checkpoint-4136/config.json\n",
      "Model weights saved in ./results_6/checkpoint-4136/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_6/checkpoint-4136/tokenizer_config.json\n",
      "Special tokens file saved in ./results_6/checkpoint-4136/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_6/checkpoint-4512\n",
      "Configuration saved in ./results_6/checkpoint-4512/config.json\n",
      "Model weights saved in ./results_6/checkpoint-4512/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_6/checkpoint-4512/tokenizer_config.json\n",
      "Special tokens file saved in ./results_6/checkpoint-4512/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_6/checkpoint-4888\n",
      "Configuration saved in ./results_6/checkpoint-4888/config.json\n",
      "Model weights saved in ./results_6/checkpoint-4888/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_6/checkpoint-4888/tokenizer_config.json\n",
      "Special tokens file saved in ./results_6/checkpoint-4888/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_6/checkpoint-5264\n",
      "Configuration saved in ./results_6/checkpoint-5264/config.json\n",
      "Model weights saved in ./results_6/checkpoint-5264/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_6/checkpoint-5264/tokenizer_config.json\n",
      "Special tokens file saved in ./results_6/checkpoint-5264/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_6/checkpoint-5640\n",
      "Configuration saved in ./results_6/checkpoint-5640/config.json\n",
      "Model weights saved in ./results_6/checkpoint-5640/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_6/checkpoint-5640/tokenizer_config.json\n",
      "Special tokens file saved in ./results_6/checkpoint-5640/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_6/checkpoint-5640 (score: 0.8947953581809998).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4496\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3245\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9281078577041626, 'eval_accuracy': 0.8947953581809998, 'eval_runtime': 153.9566, 'eval_samples_per_second': 29.203, 'eval_steps_per_second': 1.825, 'epoch': 15.0}\n",
      "Test metrics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./trained_model_6/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.3944590091705322, 'eval_accuracy': 0.6027734875679016, 'eval_runtime': 110.6782, 'eval_samples_per_second': 29.319, 'eval_steps_per_second': 1.834, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./trained_model_6/pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model_6/tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model_6/special_tokens_map.json\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3245\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute '_format_native_types'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 147\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Concatenate logits DataFrame with the results_df\u001b[39;00m\n\u001b[1;32m    146\u001b[0m final_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([results_df, logits_df], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m \u001b[43mfinal_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_predictions_with_logits_6.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Filter out misclassified samples\u001b[39;00m\n\u001b[1;32m    150\u001b[0m misclassified_df \u001b[38;5;241m=\u001b[39m results_df[results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_pred\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    332\u001b[0m kind \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mParameter\u001b[38;5;241m.\u001b[39mPOSITIONAL_OR_KEYWORD\n\u001b[0;32m--> 333\u001b[0m params \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    334\u001b[0m     inspect\u001b[38;5;241m.\u001b[39mParameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, kind),\n\u001b[1;32m    335\u001b[0m     inspect\u001b[38;5;241m.\u001b[39mParameter(name, kind, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    336\u001b[0m     inspect\u001b[38;5;241m.\u001b[39mParameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m, kind, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    337\u001b[0m     inspect\u001b[38;5;241m.\u001b[39mParameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m, kind, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    338\u001b[0m     inspect\u001b[38;5;241m.\u001b[39mParameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m\"\u001b[39m, kind, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    339\u001b[0m ]\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pname, default \u001b[38;5;129;01min\u001b[39;00m extra_params:\n\u001b[1;32m    342\u001b[0m     params\u001b[38;5;241m.\u001b[39mappend(inspect\u001b[38;5;241m.\u001b[39mParameter(pname, kind, default\u001b[38;5;241m=\u001b[39mdefault))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3909\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   3910\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mxs\u001b[39m(\n\u001b[1;32m   3911\u001b[0m     \u001b[38;5;28mself\u001b[39m: NDFrameT,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3915\u001b[0m     drop_level: bool_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   3916\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[1;32m   3917\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3918\u001b[0m \u001b[38;5;124;03m    Return cross-section from the Series/DataFrame.\u001b[39;00m\n\u001b[1;32m   3919\u001b[0m \n\u001b[1;32m   3920\u001b[0m \u001b[38;5;124;03m    This method takes a `key` argument to select data at a particular\u001b[39;00m\n\u001b[1;32m   3921\u001b[0m \u001b[38;5;124;03m    level of a MultiIndex.\u001b[39;00m\n\u001b[1;32m   3922\u001b[0m \n\u001b[1;32m   3923\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   3924\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m   3925\u001b[0m \u001b[38;5;124;03m    key : label or tuple of label\u001b[39;00m\n\u001b[1;32m   3926\u001b[0m \u001b[38;5;124;03m        Label contained in the index, or partially in a MultiIndex.\u001b[39;00m\n\u001b[1;32m   3927\u001b[0m \u001b[38;5;124;03m    axis : {0 or 'index', 1 or 'columns'}, default 0\u001b[39;00m\n\u001b[1;32m   3928\u001b[0m \u001b[38;5;124;03m        Axis to retrieve cross-section on.\u001b[39;00m\n\u001b[1;32m   3929\u001b[0m \u001b[38;5;124;03m    level : object, defaults to first n levels (n=1 or len(key))\u001b[39;00m\n\u001b[1;32m   3930\u001b[0m \u001b[38;5;124;03m        In case of a key partially contained in a MultiIndex, indicate\u001b[39;00m\n\u001b[1;32m   3931\u001b[0m \u001b[38;5;124;03m        which levels are used. Levels can be referred by label or position.\u001b[39;00m\n\u001b[1;32m   3932\u001b[0m \u001b[38;5;124;03m    drop_level : bool, default True\u001b[39;00m\n\u001b[1;32m   3933\u001b[0m \u001b[38;5;124;03m        If False, returns object with same levels as self.\u001b[39;00m\n\u001b[1;32m   3934\u001b[0m \n\u001b[1;32m   3935\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[1;32m   3936\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[1;32m   3937\u001b[0m \u001b[38;5;124;03m    Series or DataFrame\u001b[39;00m\n\u001b[1;32m   3938\u001b[0m \u001b[38;5;124;03m        Cross-section from the original Series or DataFrame\u001b[39;00m\n\u001b[1;32m   3939\u001b[0m \u001b[38;5;124;03m        corresponding to the selected index levels.\u001b[39;00m\n\u001b[1;32m   3940\u001b[0m \n\u001b[1;32m   3941\u001b[0m \u001b[38;5;124;03m    See Also\u001b[39;00m\n\u001b[1;32m   3942\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[1;32m   3943\u001b[0m \u001b[38;5;124;03m    DataFrame.loc : Access a group of rows and columns\u001b[39;00m\n\u001b[1;32m   3944\u001b[0m \u001b[38;5;124;03m        by label(s) or a boolean array.\u001b[39;00m\n\u001b[1;32m   3945\u001b[0m \u001b[38;5;124;03m    DataFrame.iloc : Purely integer-location based indexing\u001b[39;00m\n\u001b[1;32m   3946\u001b[0m \u001b[38;5;124;03m        for selection by position.\u001b[39;00m\n\u001b[1;32m   3947\u001b[0m \n\u001b[1;32m   3948\u001b[0m \u001b[38;5;124;03m    Notes\u001b[39;00m\n\u001b[1;32m   3949\u001b[0m \u001b[38;5;124;03m    -----\u001b[39;00m\n\u001b[1;32m   3950\u001b[0m \u001b[38;5;124;03m    `xs` can not be used to set values.\u001b[39;00m\n\u001b[1;32m   3951\u001b[0m \n\u001b[1;32m   3952\u001b[0m \u001b[38;5;124;03m    MultiIndex Slicers is a generic way to get/set values on\u001b[39;00m\n\u001b[1;32m   3953\u001b[0m \u001b[38;5;124;03m    any level or levels.\u001b[39;00m\n\u001b[1;32m   3954\u001b[0m \u001b[38;5;124;03m    It is a superset of `xs` functionality, see\u001b[39;00m\n\u001b[1;32m   3955\u001b[0m \u001b[38;5;124;03m    :ref:`MultiIndex Slicers <advanced.mi_slicers>`.\u001b[39;00m\n\u001b[1;32m   3956\u001b[0m \n\u001b[1;32m   3957\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[1;32m   3958\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[1;32m   3959\u001b[0m \u001b[38;5;124;03m    >>> d = {'num_legs': [4, 4, 2, 2],\u001b[39;00m\n\u001b[1;32m   3960\u001b[0m \u001b[38;5;124;03m    ...      'num_wings': [0, 0, 2, 2],\u001b[39;00m\n\u001b[1;32m   3961\u001b[0m \u001b[38;5;124;03m    ...      'class': ['mammal', 'mammal', 'mammal', 'bird'],\u001b[39;00m\n\u001b[1;32m   3962\u001b[0m \u001b[38;5;124;03m    ...      'animal': ['cat', 'dog', 'bat', 'penguin'],\u001b[39;00m\n\u001b[1;32m   3963\u001b[0m \u001b[38;5;124;03m    ...      'locomotion': ['walks', 'walks', 'flies', 'walks']}\u001b[39;00m\n\u001b[1;32m   3964\u001b[0m \u001b[38;5;124;03m    >>> df = pd.DataFrame(data=d)\u001b[39;00m\n\u001b[1;32m   3965\u001b[0m \u001b[38;5;124;03m    >>> df = df.set_index(['class', 'animal', 'locomotion'])\u001b[39;00m\n\u001b[1;32m   3966\u001b[0m \u001b[38;5;124;03m    >>> df\u001b[39;00m\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;124;03m                               num_legs  num_wings\u001b[39;00m\n\u001b[1;32m   3968\u001b[0m \u001b[38;5;124;03m    class  animal  locomotion\u001b[39;00m\n\u001b[1;32m   3969\u001b[0m \u001b[38;5;124;03m    mammal cat     walks              4          0\u001b[39;00m\n\u001b[1;32m   3970\u001b[0m \u001b[38;5;124;03m           dog     walks              4          0\u001b[39;00m\n\u001b[1;32m   3971\u001b[0m \u001b[38;5;124;03m           bat     flies              2          2\u001b[39;00m\n\u001b[1;32m   3972\u001b[0m \u001b[38;5;124;03m    bird   penguin walks              2          2\u001b[39;00m\n\u001b[1;32m   3973\u001b[0m \n\u001b[1;32m   3974\u001b[0m \u001b[38;5;124;03m    Get values at specified index\u001b[39;00m\n\u001b[1;32m   3975\u001b[0m \n\u001b[1;32m   3976\u001b[0m \u001b[38;5;124;03m    >>> df.xs('mammal')\u001b[39;00m\n\u001b[1;32m   3977\u001b[0m \u001b[38;5;124;03m                       num_legs  num_wings\u001b[39;00m\n\u001b[1;32m   3978\u001b[0m \u001b[38;5;124;03m    animal locomotion\u001b[39;00m\n\u001b[1;32m   3979\u001b[0m \u001b[38;5;124;03m    cat    walks              4          0\u001b[39;00m\n\u001b[1;32m   3980\u001b[0m \u001b[38;5;124;03m    dog    walks              4          0\u001b[39;00m\n\u001b[1;32m   3981\u001b[0m \u001b[38;5;124;03m    bat    flies              2          2\u001b[39;00m\n\u001b[1;32m   3982\u001b[0m \n\u001b[1;32m   3983\u001b[0m \u001b[38;5;124;03m    Get values at several indexes\u001b[39;00m\n\u001b[1;32m   3984\u001b[0m \n\u001b[1;32m   3985\u001b[0m \u001b[38;5;124;03m    >>> df.xs(('mammal', 'dog'))\u001b[39;00m\n\u001b[1;32m   3986\u001b[0m \u001b[38;5;124;03m                num_legs  num_wings\u001b[39;00m\n\u001b[1;32m   3987\u001b[0m \u001b[38;5;124;03m    locomotion\u001b[39;00m\n\u001b[1;32m   3988\u001b[0m \u001b[38;5;124;03m    walks              4          0\u001b[39;00m\n\u001b[1;32m   3989\u001b[0m \n\u001b[1;32m   3990\u001b[0m \u001b[38;5;124;03m    Get values at specified index and level\u001b[39;00m\n\u001b[1;32m   3991\u001b[0m \n\u001b[1;32m   3992\u001b[0m \u001b[38;5;124;03m    >>> df.xs('cat', level=1)\u001b[39;00m\n\u001b[1;32m   3993\u001b[0m \u001b[38;5;124;03m                       num_legs  num_wings\u001b[39;00m\n\u001b[1;32m   3994\u001b[0m \u001b[38;5;124;03m    class  locomotion\u001b[39;00m\n\u001b[1;32m   3995\u001b[0m \u001b[38;5;124;03m    mammal walks              4          0\u001b[39;00m\n\u001b[1;32m   3996\u001b[0m \n\u001b[1;32m   3997\u001b[0m \u001b[38;5;124;03m    Get values at several indexes and levels\u001b[39;00m\n\u001b[1;32m   3998\u001b[0m \n\u001b[1;32m   3999\u001b[0m \u001b[38;5;124;03m    >>> df.xs(('bird', 'walks'),\u001b[39;00m\n\u001b[1;32m   4000\u001b[0m \u001b[38;5;124;03m    ...       level=[0, 'locomotion'])\u001b[39;00m\n\u001b[1;32m   4001\u001b[0m \u001b[38;5;124;03m             num_legs  num_wings\u001b[39;00m\n\u001b[1;32m   4002\u001b[0m \u001b[38;5;124;03m    animal\u001b[39;00m\n\u001b[1;32m   4003\u001b[0m \u001b[38;5;124;03m    penguin         2          2\u001b[39;00m\n\u001b[1;32m   4004\u001b[0m \n\u001b[1;32m   4005\u001b[0m \u001b[38;5;124;03m    Get values at specified column and axis\u001b[39;00m\n\u001b[1;32m   4006\u001b[0m \n\u001b[1;32m   4007\u001b[0m \u001b[38;5;124;03m    >>> df.xs('num_wings', axis=1)\u001b[39;00m\n\u001b[1;32m   4008\u001b[0m \u001b[38;5;124;03m    class   animal   locomotion\u001b[39;00m\n\u001b[1;32m   4009\u001b[0m \u001b[38;5;124;03m    mammal  cat      walks         0\u001b[39;00m\n\u001b[1;32m   4010\u001b[0m \u001b[38;5;124;03m            dog      walks         0\u001b[39;00m\n\u001b[1;32m   4011\u001b[0m \u001b[38;5;124;03m            bat      flies         2\u001b[39;00m\n\u001b[1;32m   4012\u001b[0m \u001b[38;5;124;03m    bird    penguin  walks         2\u001b[39;00m\n\u001b[1;32m   4013\u001b[0m \u001b[38;5;124;03m    Name: num_wings, dtype: int64\u001b[39;00m\n\u001b[1;32m   4014\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4015\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m   4016\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/formats/format.py:995\u001b[0m, in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     col_header \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m columns\u001b[38;5;241m.\u001b[39mnlevels\n\u001b[0;32m--> 995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader:\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m col_header \u001b[38;5;241m+\u001b[39m adjoined\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/formats/csvs.py:89\u001b[0m, in \u001b[0;36mCSVFormatter.__init__\u001b[0;34m(self, formatter, path_or_buf, sep, cols, index_label, mode, encoding, errors, compression, quoting, lineterminator, chunksize, quotechar, date_format, doublequote, escapechar, storage_options)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator \u001b[38;5;241m=\u001b[39m lineterminator \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlinesep\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdate_format \u001b[38;5;241m=\u001b[39m date_format\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_chunksize(chunksize)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/formats/csvs.py:161\u001b[0m, in \u001b[0;36mCSVFormatter._initialize_columns\u001b[0;34m(self, cols)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# update columns to include possible multiplicity of dupes\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# and make sure cols is just a list of labels\u001b[39;00m\n\u001b[1;32m    160\u001b[0m new_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_cols\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_native_types\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute '_format_native_types'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import os\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('LT-EDI-ACL2022_Dataset/train.csv')\n",
    "dev_df = pd.read_csv('LT-EDI-ACL2022_Dataset/dev.tsv', sep='\\t')\n",
    "test_df = pd.read_csv('LT-EDI-ACL2022_Dataset/test.csv')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_df.drop(\"pid\", axis=1, inplace=True)\n",
    "dev_df.drop(\"PID\", axis=1, inplace=True)\n",
    "test_df.drop(\"pid\", axis=1, inplace=True)\n",
    "\n",
    "# Rename columns for dev_df\n",
    "dev_df = dev_df.rename(columns={'Text data': 'text', 'Label': 'labels'})\n",
    "\n",
    "# Map label names to integers\n",
    "label_mapping = {'severe': 0, 'moderate': 1, 'not depression': 2}\n",
    "dev_df['labels'] = dev_df['labels'].map(label_mapping)\n",
    "\n",
    "# Convert DataFrame to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(dev_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Merge datasets into a DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Tokenizer: Load from FacebookAI/roberta-base\n",
    "model_checkpoint = \"roberta-base\"  # Changed to FacebookAI's base model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    cleaned_texts = [str(text) if text else \"\" for text in examples[\"text\"]]\n",
    "    return tokenizer(cleaned_texts, truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Load the base model (Roberta base) for sequence classification with 3 classes\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Data Collator: Use padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# Define evaluation metrics (accuracy)\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = torch.argmax(torch.tensor(predictions), dim=-1)\n",
    "    accuracy = (preds == torch.tensor(labels)).float().mean().item()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_6\",  # Updated output directory\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_6',  # Updated logging directory\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Scheduler for learning rate\n",
    "num_training_steps = len(tokenized_datasets[\"train\"]) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2) \n",
    "\n",
    "# Trainer class\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Validation metrics:\")\n",
    "val_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n",
    "print(val_metrics)\n",
    "\n",
    "print(\"Test metrics:\")\n",
    "test_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "print(test_metrics)\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "model.save_pretrained('./trained_model_6')\n",
    "tokenizer.save_pretrained('./trained_model_6')\n",
    "\n",
    "# Get predictions for the test set using the trained model\n",
    "def get_predictions_and_logits(trainer, dataset):\n",
    "    predictions, labels, metrics = trainer.predict(dataset)\n",
    "    \n",
    "    # Convert logits to predicted labels \n",
    "    predicted_labels = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    return predictions, predicted_labels, labels\n",
    "\n",
    "# Get predictions, logits, and true labels for the test dataset\n",
    "predictions, predicted_labels, true_labels = get_predictions_and_logits(trainer, tokenized_datasets[\"test\"])\n",
    "\n",
    "# Convert logits to pandas DataFrame for easy manipulation\n",
    "logits_df = pd.DataFrame(predictions, columns=[\"logit_\" + str(i) for i in range(predictions.shape[1])])\n",
    "\n",
    "# Create a DataFrame with text, true labels, and predicted labels\n",
    "results_df = pd.DataFrame({\n",
    "    'text': tokenized_datasets[\"test\"][\"text\"],\n",
    "    'label': true_labels,\n",
    "    'label_pred': predicted_labels\n",
    "})\n",
    "\n",
    "# Concatenate logits DataFrame with the results_df\n",
    "final_df = pd.concat([results_df, logits_df], axis=1)\n",
    "final_df.to_csv(\"test_predictions_with_logits_6.csv\", index=False)\n",
    "\n",
    "# Filter out misclassified samples\n",
    "misclassified_df = results_df[results_df['label'] != results_df['label_pred']]\n",
    "misclassified_df.to_csv('misclassified_samples_6.csv', index=False)\n",
    "\n",
    "# Count per label\n",
    "misclassified_counts = misclassified_df['label'].value_counts()\n",
    "\n",
    "# Display distribution of misclassified labels\n",
    "print(f\"Misclassification distribution:\\n{misclassified_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label', 'label_pred', 'logit_0', 'logit_1', 'logit_2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(final_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label label_pred   \n",
      "0  Im scared : This is it. I lie to myself every ...     1          1  \\\n",
      "1  New to this but just wanted to vent : I just f...     1          1   \n",
      "2  I’m sad : It’s kinda always been an issue. I w...     1          2   \n",
      "3  Lonely but not alone. : All of my immediately ...     1          1   \n",
      "4  This year has been trash. : I dont know why I’...     1          1   \n",
      "\n",
      "      logit_0      logit_1     logit_2  \n",
      "0  -4.2713637    6.9045553  -3.2796314  \n",
      "1  -4.7947664    6.3023434  -2.1778367  \n",
      "2   -5.588239  -0.31869474    5.529555  \n",
      "3  -4.2503605     6.968496  -3.3688626  \n",
      "4  -3.9920104    6.9639244  -3.6148562  \n"
     ]
    }
   ],
   "source": [
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text          0\n",
      "label         0\n",
      "label_pred    0\n",
      "logit_0       0\n",
      "logit_1       0\n",
      "logit_2       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(final_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute '_format_native_types'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m new_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(final_df\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnew_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_predictions_with_logits_6.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    332\u001b[0m kind \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mParameter\u001b[38;5;241m.\u001b[39mPOSITIONAL_OR_KEYWORD\n\u001b[0;32m--> 333\u001b[0m params \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    334\u001b[0m     inspect\u001b[38;5;241m.\u001b[39mParameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, kind),\n\u001b[1;32m    335\u001b[0m     inspect\u001b[38;5;241m.\u001b[39mParameter(name, kind, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    336\u001b[0m     inspect\u001b[38;5;241m.\u001b[39mParameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m, kind, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    337\u001b[0m     inspect\u001b[38;5;241m.\u001b[39mParameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m, kind, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    338\u001b[0m     inspect\u001b[38;5;241m.\u001b[39mParameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m\"\u001b[39m, kind, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    339\u001b[0m ]\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pname, default \u001b[38;5;129;01min\u001b[39;00m extra_params:\n\u001b[1;32m    342\u001b[0m     params\u001b[38;5;241m.\u001b[39mappend(inspect\u001b[38;5;241m.\u001b[39mParameter(pname, kind, default\u001b[38;5;241m=\u001b[39mdefault))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3909\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   3910\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mxs\u001b[39m(\n\u001b[1;32m   3911\u001b[0m     \u001b[38;5;28mself\u001b[39m: NDFrameT,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3915\u001b[0m     drop_level: bool_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   3916\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[1;32m   3917\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3918\u001b[0m \u001b[38;5;124;03m    Return cross-section from the Series/DataFrame.\u001b[39;00m\n\u001b[1;32m   3919\u001b[0m \n\u001b[1;32m   3920\u001b[0m \u001b[38;5;124;03m    This method takes a `key` argument to select data at a particular\u001b[39;00m\n\u001b[1;32m   3921\u001b[0m \u001b[38;5;124;03m    level of a MultiIndex.\u001b[39;00m\n\u001b[1;32m   3922\u001b[0m \n\u001b[1;32m   3923\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   3924\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m   3925\u001b[0m \u001b[38;5;124;03m    key : label or tuple of label\u001b[39;00m\n\u001b[1;32m   3926\u001b[0m \u001b[38;5;124;03m        Label contained in the index, or partially in a MultiIndex.\u001b[39;00m\n\u001b[1;32m   3927\u001b[0m \u001b[38;5;124;03m    axis : {0 or 'index', 1 or 'columns'}, default 0\u001b[39;00m\n\u001b[1;32m   3928\u001b[0m \u001b[38;5;124;03m        Axis to retrieve cross-section on.\u001b[39;00m\n\u001b[1;32m   3929\u001b[0m \u001b[38;5;124;03m    level : object, defaults to first n levels (n=1 or len(key))\u001b[39;00m\n\u001b[1;32m   3930\u001b[0m \u001b[38;5;124;03m        In case of a key partially contained in a MultiIndex, indicate\u001b[39;00m\n\u001b[1;32m   3931\u001b[0m \u001b[38;5;124;03m        which levels are used. Levels can be referred by label or position.\u001b[39;00m\n\u001b[1;32m   3932\u001b[0m \u001b[38;5;124;03m    drop_level : bool, default True\u001b[39;00m\n\u001b[1;32m   3933\u001b[0m \u001b[38;5;124;03m        If False, returns object with same levels as self.\u001b[39;00m\n\u001b[1;32m   3934\u001b[0m \n\u001b[1;32m   3935\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[1;32m   3936\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[1;32m   3937\u001b[0m \u001b[38;5;124;03m    Series or DataFrame\u001b[39;00m\n\u001b[1;32m   3938\u001b[0m \u001b[38;5;124;03m        Cross-section from the original Series or DataFrame\u001b[39;00m\n\u001b[1;32m   3939\u001b[0m \u001b[38;5;124;03m        corresponding to the selected index levels.\u001b[39;00m\n\u001b[1;32m   3940\u001b[0m \n\u001b[1;32m   3941\u001b[0m \u001b[38;5;124;03m    See Also\u001b[39;00m\n\u001b[1;32m   3942\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[1;32m   3943\u001b[0m \u001b[38;5;124;03m    DataFrame.loc : Access a group of rows and columns\u001b[39;00m\n\u001b[1;32m   3944\u001b[0m \u001b[38;5;124;03m        by label(s) or a boolean array.\u001b[39;00m\n\u001b[1;32m   3945\u001b[0m \u001b[38;5;124;03m    DataFrame.iloc : Purely integer-location based indexing\u001b[39;00m\n\u001b[1;32m   3946\u001b[0m \u001b[38;5;124;03m        for selection by position.\u001b[39;00m\n\u001b[1;32m   3947\u001b[0m \n\u001b[1;32m   3948\u001b[0m \u001b[38;5;124;03m    Notes\u001b[39;00m\n\u001b[1;32m   3949\u001b[0m \u001b[38;5;124;03m    -----\u001b[39;00m\n\u001b[1;32m   3950\u001b[0m \u001b[38;5;124;03m    `xs` can not be used to set values.\u001b[39;00m\n\u001b[1;32m   3951\u001b[0m \n\u001b[1;32m   3952\u001b[0m \u001b[38;5;124;03m    MultiIndex Slicers is a generic way to get/set values on\u001b[39;00m\n\u001b[1;32m   3953\u001b[0m \u001b[38;5;124;03m    any level or levels.\u001b[39;00m\n\u001b[1;32m   3954\u001b[0m \u001b[38;5;124;03m    It is a superset of `xs` functionality, see\u001b[39;00m\n\u001b[1;32m   3955\u001b[0m \u001b[38;5;124;03m    :ref:`MultiIndex Slicers <advanced.mi_slicers>`.\u001b[39;00m\n\u001b[1;32m   3956\u001b[0m \n\u001b[1;32m   3957\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[1;32m   3958\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[1;32m   3959\u001b[0m \u001b[38;5;124;03m    >>> d = {'num_legs': [4, 4, 2, 2],\u001b[39;00m\n\u001b[1;32m   3960\u001b[0m \u001b[38;5;124;03m    ...      'num_wings': [0, 0, 2, 2],\u001b[39;00m\n\u001b[1;32m   3961\u001b[0m \u001b[38;5;124;03m    ...      'class': ['mammal', 'mammal', 'mammal', 'bird'],\u001b[39;00m\n\u001b[1;32m   3962\u001b[0m \u001b[38;5;124;03m    ...      'animal': ['cat', 'dog', 'bat', 'penguin'],\u001b[39;00m\n\u001b[1;32m   3963\u001b[0m \u001b[38;5;124;03m    ...      'locomotion': ['walks', 'walks', 'flies', 'walks']}\u001b[39;00m\n\u001b[1;32m   3964\u001b[0m \u001b[38;5;124;03m    >>> df = pd.DataFrame(data=d)\u001b[39;00m\n\u001b[1;32m   3965\u001b[0m \u001b[38;5;124;03m    >>> df = df.set_index(['class', 'animal', 'locomotion'])\u001b[39;00m\n\u001b[1;32m   3966\u001b[0m \u001b[38;5;124;03m    >>> df\u001b[39;00m\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;124;03m                               num_legs  num_wings\u001b[39;00m\n\u001b[1;32m   3968\u001b[0m \u001b[38;5;124;03m    class  animal  locomotion\u001b[39;00m\n\u001b[1;32m   3969\u001b[0m \u001b[38;5;124;03m    mammal cat     walks              4          0\u001b[39;00m\n\u001b[1;32m   3970\u001b[0m \u001b[38;5;124;03m           dog     walks              4          0\u001b[39;00m\n\u001b[1;32m   3971\u001b[0m \u001b[38;5;124;03m           bat     flies              2          2\u001b[39;00m\n\u001b[1;32m   3972\u001b[0m \u001b[38;5;124;03m    bird   penguin walks              2          2\u001b[39;00m\n\u001b[1;32m   3973\u001b[0m \n\u001b[1;32m   3974\u001b[0m \u001b[38;5;124;03m    Get values at specified index\u001b[39;00m\n\u001b[1;32m   3975\u001b[0m \n\u001b[1;32m   3976\u001b[0m \u001b[38;5;124;03m    >>> df.xs('mammal')\u001b[39;00m\n\u001b[1;32m   3977\u001b[0m \u001b[38;5;124;03m                       num_legs  num_wings\u001b[39;00m\n\u001b[1;32m   3978\u001b[0m \u001b[38;5;124;03m    animal locomotion\u001b[39;00m\n\u001b[1;32m   3979\u001b[0m \u001b[38;5;124;03m    cat    walks              4          0\u001b[39;00m\n\u001b[1;32m   3980\u001b[0m \u001b[38;5;124;03m    dog    walks              4          0\u001b[39;00m\n\u001b[1;32m   3981\u001b[0m \u001b[38;5;124;03m    bat    flies              2          2\u001b[39;00m\n\u001b[1;32m   3982\u001b[0m \n\u001b[1;32m   3983\u001b[0m \u001b[38;5;124;03m    Get values at several indexes\u001b[39;00m\n\u001b[1;32m   3984\u001b[0m \n\u001b[1;32m   3985\u001b[0m \u001b[38;5;124;03m    >>> df.xs(('mammal', 'dog'))\u001b[39;00m\n\u001b[1;32m   3986\u001b[0m \u001b[38;5;124;03m                num_legs  num_wings\u001b[39;00m\n\u001b[1;32m   3987\u001b[0m \u001b[38;5;124;03m    locomotion\u001b[39;00m\n\u001b[1;32m   3988\u001b[0m \u001b[38;5;124;03m    walks              4          0\u001b[39;00m\n\u001b[1;32m   3989\u001b[0m \n\u001b[1;32m   3990\u001b[0m \u001b[38;5;124;03m    Get values at specified index and level\u001b[39;00m\n\u001b[1;32m   3991\u001b[0m \n\u001b[1;32m   3992\u001b[0m \u001b[38;5;124;03m    >>> df.xs('cat', level=1)\u001b[39;00m\n\u001b[1;32m   3993\u001b[0m \u001b[38;5;124;03m                       num_legs  num_wings\u001b[39;00m\n\u001b[1;32m   3994\u001b[0m \u001b[38;5;124;03m    class  locomotion\u001b[39;00m\n\u001b[1;32m   3995\u001b[0m \u001b[38;5;124;03m    mammal walks              4          0\u001b[39;00m\n\u001b[1;32m   3996\u001b[0m \n\u001b[1;32m   3997\u001b[0m \u001b[38;5;124;03m    Get values at several indexes and levels\u001b[39;00m\n\u001b[1;32m   3998\u001b[0m \n\u001b[1;32m   3999\u001b[0m \u001b[38;5;124;03m    >>> df.xs(('bird', 'walks'),\u001b[39;00m\n\u001b[1;32m   4000\u001b[0m \u001b[38;5;124;03m    ...       level=[0, 'locomotion'])\u001b[39;00m\n\u001b[1;32m   4001\u001b[0m \u001b[38;5;124;03m             num_legs  num_wings\u001b[39;00m\n\u001b[1;32m   4002\u001b[0m \u001b[38;5;124;03m    animal\u001b[39;00m\n\u001b[1;32m   4003\u001b[0m \u001b[38;5;124;03m    penguin         2          2\u001b[39;00m\n\u001b[1;32m   4004\u001b[0m \n\u001b[1;32m   4005\u001b[0m \u001b[38;5;124;03m    Get values at specified column and axis\u001b[39;00m\n\u001b[1;32m   4006\u001b[0m \n\u001b[1;32m   4007\u001b[0m \u001b[38;5;124;03m    >>> df.xs('num_wings', axis=1)\u001b[39;00m\n\u001b[1;32m   4008\u001b[0m \u001b[38;5;124;03m    class   animal   locomotion\u001b[39;00m\n\u001b[1;32m   4009\u001b[0m \u001b[38;5;124;03m    mammal  cat      walks         0\u001b[39;00m\n\u001b[1;32m   4010\u001b[0m \u001b[38;5;124;03m            dog      walks         0\u001b[39;00m\n\u001b[1;32m   4011\u001b[0m \u001b[38;5;124;03m            bat      flies         2\u001b[39;00m\n\u001b[1;32m   4012\u001b[0m \u001b[38;5;124;03m    bird    penguin  walks         2\u001b[39;00m\n\u001b[1;32m   4013\u001b[0m \u001b[38;5;124;03m    Name: num_wings, dtype: int64\u001b[39;00m\n\u001b[1;32m   4014\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4015\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m   4016\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/formats/format.py:995\u001b[0m, in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     col_header \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m columns\u001b[38;5;241m.\u001b[39mnlevels\n\u001b[0;32m--> 995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader:\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m col_header \u001b[38;5;241m+\u001b[39m adjoined\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/formats/csvs.py:89\u001b[0m, in \u001b[0;36mCSVFormatter.__init__\u001b[0;34m(self, formatter, path_or_buf, sep, cols, index_label, mode, encoding, errors, compression, quoting, lineterminator, chunksize, quotechar, date_format, doublequote, escapechar, storage_options)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator \u001b[38;5;241m=\u001b[39m lineterminator \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlinesep\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdate_format \u001b[38;5;241m=\u001b[39m date_format\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_chunksize(chunksize)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/formats/csvs.py:161\u001b[0m, in \u001b[0;36mCSVFormatter._initialize_columns\u001b[0;34m(self, cols)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# update columns to include possible multiplicity of dupes\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# and make sure cols is just a list of labels\u001b[39;00m\n\u001b[1;32m    160\u001b[0m new_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_cols\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_native_types\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute '_format_native_types'"
     ]
    }
   ],
   "source": [
    "new_df = pd.DataFrame(final_df.copy())\n",
    "new_df.to_csv(\"test_predictions_with_logits_6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification distribution:\n",
      "label\n",
      "1    753\n",
      "2    422\n",
      "0    114\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter out misclassified samples\n",
    "misclassified_df = results_df[results_df['label'] != results_df['label_pred']]\n",
    "# misclassified_df.to_csv('misclassified_samples_6.csv', index=False)\n",
    "\n",
    "# Count per label\n",
    "misclassified_counts = misclassified_df['label'].value_counts()\n",
    "\n",
    "# Display distribution of misclassified labels\n",
    "print(f\"Misclassification distribution:\\n{misclassified_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No. 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0fb6d42b644313bb8f19ec8cbec465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e2e6753c544c44a5fcbe60aeb9493f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb1ac63c59b47ebae4374d063a57a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./trained_model_6/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./trained_model_6/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./trained_model_6 and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 6184\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5805\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2322' max='5805' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2322/5805 1:11:41 < 1:47:38, 0.54 it/s, Epoch 6/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.270800</td>\n",
       "      <td>0.136914</td>\n",
       "      <td>0.958603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.187900</td>\n",
       "      <td>0.127784</td>\n",
       "      <td>0.965071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.094100</td>\n",
       "      <td>0.111236</td>\n",
       "      <td>0.972833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.135969</td>\n",
       "      <td>0.976714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.203774</td>\n",
       "      <td>0.967658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.197683</td>\n",
       "      <td>0.972833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 773\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_8/checkpoint-387\n",
      "Configuration saved in ./results_8/checkpoint-387/config.json\n",
      "Model weights saved in ./results_8/checkpoint-387/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_8/checkpoint-387/tokenizer_config.json\n",
      "Special tokens file saved in ./results_8/checkpoint-387/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 773\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_8/checkpoint-774\n",
      "Configuration saved in ./results_8/checkpoint-774/config.json\n",
      "Model weights saved in ./results_8/checkpoint-774/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_8/checkpoint-774/tokenizer_config.json\n",
      "Special tokens file saved in ./results_8/checkpoint-774/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 773\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_8/checkpoint-1161\n",
      "Configuration saved in ./results_8/checkpoint-1161/config.json\n",
      "Model weights saved in ./results_8/checkpoint-1161/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_8/checkpoint-1161/tokenizer_config.json\n",
      "Special tokens file saved in ./results_8/checkpoint-1161/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 773\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_8/checkpoint-1548\n",
      "Configuration saved in ./results_8/checkpoint-1548/config.json\n",
      "Model weights saved in ./results_8/checkpoint-1548/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_8/checkpoint-1548/tokenizer_config.json\n",
      "Special tokens file saved in ./results_8/checkpoint-1548/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 773\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_8/checkpoint-1935\n",
      "Configuration saved in ./results_8/checkpoint-1935/config.json\n",
      "Model weights saved in ./results_8/checkpoint-1935/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_8/checkpoint-1935/tokenizer_config.json\n",
      "Special tokens file saved in ./results_8/checkpoint-1935/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 773\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results_8/checkpoint-2322\n",
      "Configuration saved in ./results_8/checkpoint-2322/config.json\n",
      "Model weights saved in ./results_8/checkpoint-2322/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_8/checkpoint-2322/tokenizer_config.json\n",
      "Special tokens file saved in ./results_8/checkpoint-2322/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_8/checkpoint-1548 (score: 0.9767140746116638).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 773\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 774\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1359693557024002, 'eval_accuracy': 0.9767140746116638, 'eval_runtime': 24.9891, 'eval_samples_per_second': 30.933, 'eval_steps_per_second': 1.961, 'epoch': 6.0}\n",
      "Test metrics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./trained_model_8/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17540614306926727, 'eval_accuracy': 0.9689922332763672, 'eval_runtime': 24.8513, 'eval_samples_per_second': 31.145, 'eval_steps_per_second': 1.972, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./trained_model_8/pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model_8/tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model_8/special_tokens_map.json\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 774\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification distribution:\n",
      "label\n",
      "1    753\n",
      "2    422\n",
      "0    114\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('Preprocessed_Dataset/train.csv')\n",
    "val_df = pd.read_csv('Preprocessed_Dataset/dev.csv')\n",
    "test_df = pd.read_csv('Preprocessed_Dataset/test.csv')\n",
    "\n",
    "# Convert DataFrame to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Merge datasets\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Tokenizer: Load from last training\n",
    "model_checkpoint = './trained_model_6'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Tokenize\n",
    "def preprocess_function(examples):\n",
    "    cleaned_texts = [str(text) if text else \"\" for text in examples[\"text\"]]\n",
    "    return tokenizer(cleaned_texts, truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Load the model\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2, ignore_mismatched_sizes=True)  # 2 classes\n",
    "\n",
    "# Data Collector\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# Evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = torch.argmax(torch.tensor(predictions), dim=-1)\n",
    "    accuracy = (preds == torch.tensor(labels)).float().mean().item()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_8\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_8',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# learning rate scheduler\n",
    "num_training_steps = len(tokenized_datasets[\"train\"]) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=num_training_steps)\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2) \n",
    "\n",
    "# Trainer class\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Validation metrics:\")\n",
    "val_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n",
    "print(val_metrics)\n",
    "\n",
    "print(\"Test metrics:\")\n",
    "test_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "print(test_metrics)\n",
    "\n",
    "# Save\n",
    "model.save_pretrained('./trained_model_8')\n",
    "tokenizer.save_pretrained('./trained_model_8')\n",
    "\n",
    "# Get predictions for the test set using the trained model\n",
    "def get_predictions_and_logits(trainer, dataset):\n",
    "    predictions, labels, metrics = trainer.predict(dataset)\n",
    "    \n",
    "    # Convert logits to predicted labels (0 or 1)\n",
    "    predicted_labels = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    return predictions, predicted_labels, labels\n",
    "\n",
    "# Get predictions, logits, and true labels for the test dataset\n",
    "predictions, predicted_labels, true_labels = get_predictions_and_logits(trainer, tokenized_datasets[\"test\"])\n",
    "\n",
    "# Convert logits to pandas DataFrame for easy manipulation\n",
    "logits_df_8 = pd.DataFrame(predictions, columns=[\"logit_\" + str(i) for i in range(predictions.shape[1])])\n",
    "\n",
    "# Create a DataFrame with text, true labels, and predicted labels\n",
    "results_df_8 = pd.DataFrame({\n",
    "    'text': tokenized_datasets[\"test\"][\"text\"],\n",
    "    'label': true_labels,\n",
    "    'label_pred': predicted_labels\n",
    "})\n",
    "\n",
    "# Concatenate logits DataFrame with the results_df\n",
    "final_df_8 = pd.concat([results_df_8, logits_df_8], axis=1)\n",
    "# final_df_8.to_csv(\"test_predictions_with_logits_8.csv\", index=False)\n",
    "\n",
    "# Filter out misclassified samples\n",
    "misclassified_df_8 = results_df_8[results_df_8['label'] != results_df_8['label_pred']]\n",
    "# misclassified_df_8.to_csv('misclassified_samples_8.csv', index=False)\n",
    "\n",
    "# Count per label\n",
    "misclassified_counts_8 = misclassified_df_8['label'].value_counts()\n",
    "\n",
    "# Display distribution\n",
    "print(f\"Misclassification distribution:\\n{misclassified_counts_8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification distribution:\n",
      "label\n",
      "1    15\n",
      "0     9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display distribution\n",
    "print(f\"Misclassification distribution:\\n{misclassified_counts_8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02e5c984c09f435a8c9ad7fee9c0a3b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5e5b1574b89f4266bf5069a75bfd1259",
       "IPY_MODEL_6b969a0ec21d44afbb294752e0829879",
       "IPY_MODEL_5e79822663234199bbe63f1bb3b07739"
      ],
      "layout": "IPY_MODEL_27888e6192c74a13b51525f5b069246c"
     }
    },
    "18ef0f43fc8c407db9e93fc2eb456f09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1d5cf447c04047a5b763e2258918a5a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1fac47e82d8445fe9c061c5c817b7c07": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27888e6192c74a13b51525f5b069246c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f0301ad571c43f4a3f6c74ee11478ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4184657ba94e4390a12546a75189b23c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f0d6b2d68e24d4f92eecdf40266dbb5",
      "placeholder": "​",
      "style": "IPY_MODEL_1d5cf447c04047a5b763e2258918a5a3",
      "value": " 774/774 [00:00&lt;00:00, 1778.11 examples/s]"
     }
    },
    "43d38d94d64843c2822fcb105c44b9ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49adb58f54964934ab25de27be100fb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c8fdd7ab5e5412989b1eebed2cb0f32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "570858b160114bd7989322f8171be5f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e5b1574b89f4266bf5069a75bfd1259": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f0301ad571c43f4a3f6c74ee11478ae",
      "placeholder": "​",
      "style": "IPY_MODEL_920685d2c4524821aa53546cb15ca4cc",
      "value": "Map: 100%"
     }
    },
    "5e79822663234199bbe63f1bb3b07739": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fac47e82d8445fe9c061c5c817b7c07",
      "placeholder": "​",
      "style": "IPY_MODEL_8aed4c454d1f4264aaabaa1d9df58012",
      "value": " 773/773 [00:00&lt;00:00, 1801.24 examples/s]"
     }
    },
    "689163b20cda4097ace66a99862d6c61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_947d37a10f674733897f7c062feff1e6",
      "max": 6184,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_799015bd31c8441f9de61f77bd381563",
      "value": 6184
     }
    },
    "6b969a0ec21d44afbb294752e0829879": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49adb58f54964934ab25de27be100fb2",
      "max": 773,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c248980abfef4874ab0003a5c5fe767d",
      "value": 773
     }
    },
    "6da44f13a284451ca8c4bf68c3d8b421": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f92da9b60b0b495f8b937c9bda1c7f6a",
       "IPY_MODEL_689163b20cda4097ace66a99862d6c61",
       "IPY_MODEL_fabf53cd91ca4a1e89567bf7c6d47c49"
      ],
      "layout": "IPY_MODEL_8d3f09e85c57469fbbe12a566097431b"
     }
    },
    "799015bd31c8441f9de61f77bd381563": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8266a18427f249d4b25c6ac3ff87f33b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "85fbfe4074c1472ea5d67c9cffabb38c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87ee335a83a24c3d9df0668f5590f94b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c4679acd51ed487db62902f93b68597d",
       "IPY_MODEL_f38daed0ecee4e5f93f95435feca0992",
       "IPY_MODEL_4184657ba94e4390a12546a75189b23c"
      ],
      "layout": "IPY_MODEL_abd7f1c0737247e48a2862bbbee86b74"
     }
    },
    "8aed4c454d1f4264aaabaa1d9df58012": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d3f09e85c57469fbbe12a566097431b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f0d6b2d68e24d4f92eecdf40266dbb5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "920685d2c4524821aa53546cb15ca4cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "947d37a10f674733897f7c062feff1e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abd7f1c0737247e48a2862bbbee86b74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c248980abfef4874ab0003a5c5fe767d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c4679acd51ed487db62902f93b68597d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d764d603b0704c9890291f2e2db0e76a",
      "placeholder": "​",
      "style": "IPY_MODEL_570858b160114bd7989322f8171be5f5",
      "value": "Map: 100%"
     }
    },
    "d764d603b0704c9890291f2e2db0e76a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da47dee2ce5d47c99e5b55d2e61be352": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f38daed0ecee4e5f93f95435feca0992": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43d38d94d64843c2822fcb105c44b9ff",
      "max": 774,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8266a18427f249d4b25c6ac3ff87f33b",
      "value": 774
     }
    },
    "f92da9b60b0b495f8b937c9bda1c7f6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85fbfe4074c1472ea5d67c9cffabb38c",
      "placeholder": "​",
      "style": "IPY_MODEL_da47dee2ce5d47c99e5b55d2e61be352",
      "value": "Map: 100%"
     }
    },
    "fabf53cd91ca4a1e89567bf7c6d47c49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c8fdd7ab5e5412989b1eebed2cb0f32",
      "placeholder": "​",
      "style": "IPY_MODEL_18ef0f43fc8c407db9e93fc2eb456f09",
      "value": " 6184/6184 [00:02&lt;00:00, 2540.53 examples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
